{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_recall_curve\n\u001b[0;32m      4\u001b[0m \u001b[39m# Sample precision and recall values for Algorithm A and Algorithm B\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Sample precision and recall values for Algorithm A and Algorithm B\n",
    "precision_algoA = [0.95, 0.92, 0.89, 0.86, 0.82, 0.78]\n",
    "recall_algoA = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "\n",
    "precision_algoB = [0.88, 0.85, 0.80, 0.75, 0.72, 0.68]\n",
    "recall_algoB = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "# Plot the precision-recall curve for Algorithm A\n",
    "plt.plot(recall_algoA, precision_algoA, label='Algorithm A')\n",
    "\n",
    "# Plot the precision-recall curve for Algorithm B\n",
    "plt.plot(recall_algoB, precision_algoB, label='Algorithm B')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataset parameters\n",
    "parser.add_argument('data_dir', metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--dataset', '-d', metavar='NAME', default='',\n",
    "                    help='dataset type (default: ImageFolder/ImageTar if empty)')\n",
    "parser.add_argument('--train-split', metavar='NAME', default='train',\n",
    "                    help='dataset train split (default: train)')\n",
    "parser.add_argument('--val-split', metavar='NAME', default='validation',\n",
    "                    help='dataset validation split (default: validation)')\n",
    "parser.add_argument('--dataset-download', action='store_true', default=False,\n",
    "                    help='Allow download of dataset for torch/ and tfds/ datasets that support it.')\n",
    "parser.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n",
    "                    help='path to class to idx mapping file (default: \"\")')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--model', default='resnet50', type=str, metavar='MODEL',\n",
    "                    help='Name of model to train (default: \"resnet50\"')\n",
    "parser.add_argument('--pretrained', action='store_true', default=False,\n",
    "                    help='Start with pretrained version of specified network (if avail)')\n",
    "parser.add_argument('--initial-checkpoint', default='', type=str, metavar='PATH',\n",
    "                    help='Initialize model from this checkpoint (default: none)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='Resume full model and optimizer state from checkpoint (default: none)')\n",
    "parser.add_argument('--no-resume-opt', action='store_true', default=False,\n",
    "                    help='prevent resume of optimizer state when resuming model')\n",
    "parser.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                    help='number of label classes (Model default if None)')\n",
    "parser.add_argument('--gp', default=None, type=str, metavar='POOL',\n",
    "                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\n",
    "parser.add_argument('--img-size', type=int, default=None, metavar='N',\n",
    "                    help='Image patch size (default: None => model default)')\n",
    "parser.add_argument('--input-size', default=None, nargs=3, type=int,\n",
    "                    metavar='N N N', help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\n",
    "parser.add_argument('--crop-pct', default=None, type=float,\n",
    "                    metavar='N', help='Input image center crop percent (for validation only)')\n",
    "parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                    help='Override mean pixel value of dataset')\n",
    "parser.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n",
    "                    help='Override std deviation of of dataset')\n",
    "parser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n",
    "                    help='Image resize interpolation type (overrides model)')\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('-vb', '--validation-batch-size', type=int, default=None, metavar='N',\n",
    "                    help='validation batch size override (default: None)')\n",
    "\n",
    "# Optimizer parameters\n",
    "parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                    help='Optimizer (default: \"adamw\"')\n",
    "parser.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n",
    "                    help='Optimizer Epsilon (default: None, use opt default)')\n",
    "parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                    help='Optimizer Betas (default: None, use opt default)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='Optimizer momentum (default: 0.9)')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.05,\n",
    "                    help='weight decay (default: 0.05)')\n",
    "parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                    help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--clip-mode', type=str, default='norm',\n",
    "                    help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\n",
    "\n",
    "\n",
    "# Learning rate schedule parameters\n",
    "parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n",
    "                    help='LR scheduler (default: \"step\"')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                    help='learning rate noise on/off epoch percentages')\n",
    "parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                    help='learning rate noise limit percent (default: 0.67)')\n",
    "parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                    help='learning rate noise std-dev (default: 1.0)')\n",
    "parser.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n",
    "                    help='learning rate cycle len multiplier (default: 1.0)')\n",
    "parser.add_argument('--lr-cycle-decay', type=float, default=0.5, metavar='MULT',\n",
    "                    help='amount to decay each learning rate cycle (default: 0.5)')\n",
    "parser.add_argument('--lr-cycle-limit', type=int, default=1, metavar='N',\n",
    "                    help='learning rate cycle limit, cycles enabled if > 1')\n",
    "parser.add_argument('--lr-k-decay', type=float, default=1.0,\n",
    "                    help='learning rate k-decay for cosine/poly (default: 1.0)')\n",
    "parser.add_argument('--warmup-lr', type=float, default=1e-6, metavar='LR',\n",
    "                    help='warmup learning rate (default: 1e-6)')\n",
    "parser.add_argument('--min-lr', type=float, default=1e-6, metavar='LR',\n",
    "                    help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "parser.add_argument('--epochs', type=int, default=300, metavar='N',\n",
    "                    help='number of epochs to train (default: 300)')\n",
    "parser.add_argument('--epoch-repeats', type=float, default=0., metavar='N',\n",
    "                    help='epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).')\n",
    "parser.add_argument('--start-epoch', default=None, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--decay-epochs', type=float, default=100, metavar='N',\n",
    "                    help='epoch interval to decay LR')\n",
    "parser.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n",
    "                    help='epochs to warmup LR, if scheduler supports')\n",
    "parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n",
    "                    help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                    help='patience epochs for Plateau LR scheduler (default: 10')\n",
    "parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                    help='LR decay rate (default: 0.1)')\n",
    "\n",
    "# Augmentation & regularization parameters\n",
    "parser.add_argument('--no-aug', action='store_true', default=False,\n",
    "                    help='Disable all training augmentation, override other train aug args')\n",
    "parser.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], metavar='PCT',\n",
    "                    help='Random resize scale (default: 0.08 1.0)')\n",
    "parser.add_argument('--ratio', type=float, nargs='+', default=[3./4., 4./3.], metavar='RATIO',\n",
    "                    help='Random resize aspect ratio (default: 0.75 1.33)')\n",
    "parser.add_argument('--hflip', type=float, default=0.5,\n",
    "                    help='Horizontal flip training aug probability')\n",
    "parser.add_argument('--vflip', type=float, default=0.,\n",
    "                    help='Vertical flip training aug probability')\n",
    "parser.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
    "                    help='Color jitter factor (default: 0.4)')\n",
    "parser.add_argument('--aa', type=str, default=\"rand-m9-mstd0.5-inc1\", metavar='NAME',\n",
    "                    help='Use AutoAugment policy. \"v0\" or \"original\". (default: rand-m9-mstd0.5-inc1)'),\n",
    "parser.add_argument('--aug-repeats', type=int, default=0,\n",
    "                    help='Number of augmentation repetitions (distributed training only) (default: 0)')\n",
    "parser.add_argument('--aug-splits', type=int, default=0,\n",
    "                    help='Number of augmentation splits (default: 0, valid: 0 or >=2)')\n",
    "parser.add_argument('--jsd-loss', action='store_true', default=False,\n",
    "                    help='Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.')\n",
    "parser.add_argument('--bce-loss', action='store_true', default=False,\n",
    "                    help='Enable BCE loss w/ Mixup/CutMix use.')\n",
    "parser.add_argument('--bce-target-thresh', type=float, default=None,\n",
    "                    help='Threshold for binarizing softened BCE targets (default: None, disabled)')\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                    help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel',\n",
    "                    help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                    help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                    help='Do not random erase first (clean) augmentation split')\n",
    "parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                    help='mixup alpha, mixup enabled if > 0. (default: 0.8)')\n",
    "parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                    help='cutmix alpha, cutmix enabled if > 0. (default: 1.0)')\n",
    "parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                    help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                    help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                    help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "parser.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                    help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "parser.add_argument('--mixup-off-epoch', default=0, type=int, metavar='N',\n",
    "                    help='Turn off mixup after this epoch, disabled if 0 (default: 0)')\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                    help='Label smoothing (default: 0.1)')\n",
    "parser.add_argument('--train-interpolation', type=str, default='random',\n",
    "                    help='Training interpolation (random, bilinear, bicubic default: \"random\")')\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\n",
    "parser.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop path rate (default: None)')\n",
    "parser.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop block rate (default: None)')\n",
    "\n",
    "# Batch norm parameters (only works with gen_efficientnet based models currently)\n",
    "parser.add_argument('--bn-tf', action='store_true', default=False,\n",
    "                    help='Use Tensorflow BatchNorm defaults for models that support it (default: False)')\n",
    "parser.add_argument('--bn-momentum', type=float, default=None,\n",
    "                    help='BatchNorm momentum override (if not None)')\n",
    "parser.add_argument('--bn-eps', type=float, default=None,\n",
    "                    help='BatchNorm epsilon override (if not None)')\n",
    "parser.add_argument('--sync-bn', action='store_true',\n",
    "                    help='Enable NVIDIA Apex or Torch synchronized BatchNorm.')\n",
    "parser.add_argument('--dist-bn', type=str, default='reduce',\n",
    "                    help='Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")')\n",
    "parser.add_argument('--split-bn', action='store_true',\n",
    "                    help='Enable separate BN layers per augmentation split.')\n",
    "\n",
    "# Model Exponential Moving Average\n",
    "parser.add_argument('--model-ema', action='store_true', default=False,\n",
    "                    help='Enable tracking moving average of model weights')\n",
    "parser.add_argument('--model-ema-force-cpu', action='store_true', default=False,\n",
    "                    help='Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.')\n",
    "parser.add_argument('--model-ema-decay', type=float, default=0.9998,\n",
    "                    help='decay factor for model weights moving average (default: 0.9998)')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--worker-seeding', type=str, default='all',\n",
    "                    help='worker seed mode (default: all)')\n",
    "parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--recovery-interval', type=int, default=0, metavar='N',\n",
    "                    help='how many batches to wait before writing recovery checkpoint')\n",
    "parser.add_argument('--checkpoint-hist', type=int, default=10, metavar='N',\n",
    "                    help='number of checkpoints to keep (default: 10)')\n",
    "parser.add_argument('-j', '--workers', type=int, default=8, metavar='N',\n",
    "                    help='how many training processes to use (default: 8)')\n",
    "parser.add_argument('--save-images', action='store_true', default=False,\n",
    "                    help='save images of input bathes every log interval for debugging')\n",
    "parser.add_argument('--amp', action='store_true', default=False,\n",
    "                    help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\n",
    "parser.add_argument('--apex-amp', action='store_true', default=False,\n",
    "                    help='Use NVIDIA Apex AMP mixed precision')\n",
    "parser.add_argument('--native-amp', action='store_true', default=False,\n",
    "                    help='Use Native Torch AMP mixed precision')\n",
    "parser.add_argument('--no-ddp-bb', action='store_true', default=False,\n",
    "                    help='Force broadcast buffers for native DDP to off.')\n",
    "parser.add_argument('--channels-last', action='store_true', default=False,\n",
    "                    help='Use channels_last memory layout')\n",
    "parser.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no-prefetcher', action='store_true', default=False,\n",
    "                    help='disable fast prefetcher')\n",
    "parser.add_argument('--output', default='', type=str, metavar='PATH',\n",
    "                    help='path to output folder (default: none, current dir)')\n",
    "parser.add_argument('--experiment', default='', type=str, metavar='NAME',\n",
    "                    help='name of train experiment, name of sub-folder for output')\n",
    "parser.add_argument('--eval-metric', default='top1', type=str, metavar='EVAL_METRIC',\n",
    "                    help='Best metric (default: \"top1\"')\n",
    "parser.add_argument('--tta', type=int, default=0, metavar='N',\n",
    "                    help='Test/inference time augmentation (oversampling) factor. 0=None (default: 0)')\n",
    "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "parser.add_argument('--use-multi-epochs-loader', action='store_true', default=False,\n",
    "                    help='use the multi-epochs-loader to save time at the beginning of every epoch')\n",
    "parser.add_argument('--torchscript', dest='torchscript', action='store_true',\n",
    "                    help='convert model torchscript for inference')\n",
    "parser.add_argument('--log-wandb', action='store_true', default=False,\n",
    "                    help='log training and validation metrics to wandb')\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "van",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
